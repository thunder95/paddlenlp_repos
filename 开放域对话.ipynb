{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2021语言与智能技术竞赛：多技能对话\n",
    "\n",
    "多技能对话系统旨在建立一个开放域的多轮对话系统，能自然地融合多个对话技能，比如知识对话、推荐对话等，使得机器可以流畅自然地与人进行语言交互，从而有效地提升用户体验。\n",
    "\n",
    "该示例展示了如何使用PaddleNLP快速搭建[2021语言与智能技术竞赛：多技能对话](https://aistudio.baidu.com/aistudio/competition/detail/67)基线并进阶优化基线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.12.2)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp) (56.2.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n",
      "/home/aistudio/multi-skill_dialogue\n"
     ]
    }
   ],
   "source": [
    "# 安装paddlenlp最新版本\n",
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "\n",
    "%cd multi-skill_dialogue/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 多技能对话基线\n",
    "\n",
    "多技能对话比赛提供了多个子数据集，包含知识对话、推荐对话、画像对话和其他多种类型的对话数据集。基线采用UnifiedTransformer模型，模型的的输入除了数据token及`[CLS]`、`[SEP]`等special token之外，还有用于区别不同对话技能的special token。\n",
    "\n",
    "![模型输入](https://ai-studio-static-online.cdn.bcebos.com/24d697df544c4299a679e04e2d3b1442fdf17a14981e454e8a2de5c7acea8051)\n",
    "\n",
    "### 快速搭建基线Step1：数据预处理\n",
    "\n",
    "由于多技能对话比赛的[数据集](https://aistudio.baidu.com/aistudio/competition/detail/67)**数量多且数据规模大**，并且数据集之间**格式不同**，所以需要使用脚本对数据集进行预处理，同时将数据转化成id化的数据。\n",
    "\n",
    "**注意：** 需要确保脚本中的输入文件路径、输出文件路径和参数配置正确。由于数据规模较大，脚本运行时间较长(尤其是训练集)。也可自行分批次处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 注意：脚本默认只取每个数据集的部分语料进行处理作为基线模型的训练数据，参赛选手需根据需求自行修改数据处理策略\r\n",
    "# python ./tools/convert_data_to_numerical.py ./tools/spm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速搭建基线Step2：构建模型\n",
    "\n",
    "[UnifiedTransformer](https://github.com/PaddlePaddle/Knover/tree/luge-dialogue/luge-dialogue)以Transformer的编码器为网络基本组件，采用灵活的注意力机制，十分适合文本生成任务，并在模型输入中加入了标识不同对话技能的special token，使得模型能同时支持闲聊对话、推荐对话和知识对话。\n",
    "\n",
    "**PaddleNLP提供了UnifiedTransformer中文预训练模型，可以通过预训练模型名称完成一键加载。PaddleNLP为了方便用户处理数据，内置了与模型配套的Tokenizer，可以完成文本token化，token转ID，ID转token等操作。**\n",
    "\n",
    "PaddleNLP目前为UnifiedTransformer提供了两个中文预训练模型：\n",
    "- `unified_transformer-12L-cn` 该预训练模型是在大规模中文会话数据集上训练得到的\n",
    "- `unified_transformer-12L-cn-luge` 该预训练模型是`unified_transformer-12L-cn`在千言对话数据集上进行微调得到的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "[2021-07-15 12:16:43,246] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/unified_transformer-12L-cn-luge/unified_transformer-12L-cn-luge.pdparams\n",
      "[2021-07-15 12:16:54,608] [    INFO] - Found /home/aistudio/.paddlenlp/models/unified_transformer-12L-cn-luge/unified_transformer-12L-cn-vocab.txt\n",
      "[2021-07-15 12:16:54,612] [    INFO] - Found /home/aistudio/.paddlenlp/models/unified_transformer-12L-cn-luge/unified_transformer-12L-cn-spm.model\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import UnifiedTransformerLMHeadModel, UnifiedTransformerTokenizer\n",
    "\n",
    "# 预训练模型名称\n",
    "model_name_or_path = 'unified_transformer-12L-cn-luge'\n",
    "\n",
    "# 加载预训练模型\n",
    "model = UnifiedTransformerLMHeadModel.from_pretrained(model_name_or_path)\n",
    "# 加载配套的tokenizer\n",
    "tokenizer = UnifiedTransformerTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速搭建基线Step3：加载数据\n",
    "\n",
    "基线通过继承`paddle.io.IterableDataset`自定义可迭代数据集`DialogueDataset`，包括读取文件、shuffle及组batch等操作，细节详见`data.py`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle.io import DataLoader\n",
    "from data import DialogueDataset\n",
    "\n",
    "# 训练batch_size\n",
    "batch_size = 8192\n",
    "# 组batch进行排序和shuffle的pool_size\n",
    "sort_pool_size = 65536\n",
    "\n",
    "# 训练集路径，注意与数据预处理输出路径保持一致\n",
    "train_data_path = './data/train.txt' \n",
    "# 初始化Dataset\n",
    "train_dataset = DialogueDataset(\n",
    "        train_data_path,\n",
    "        batch_size,\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.cls_token_id,\n",
    "        sort_pool_size,\n",
    "        mode='train')\n",
    "# 初始化Dataloader\n",
    "train_dataloader = DataLoader(train_dataset, return_list=True, batch_size=None)\n",
    "\n",
    "# 开发集路径，注意与数据预处理输出路径保持一致\n",
    "valid_data_path = './data/dev.txt' \n",
    "valid_dataset = DialogueDataset(\n",
    "    valid_data_path,\n",
    "    batch_size,\n",
    "    tokenizer.pad_token_id,\n",
    "    tokenizer.cls_token_id,\n",
    "    sort_pool_size,\n",
    "    mode='valid')\n",
    "valid_dataloader = DataLoader(valid_dataset, return_list=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速搭建基线Step4：训练优化\n",
    "\n",
    "在该基线中，我们选择交叉熵损失函数，使用`paddle.optimizer.AdamW`作为优化器。\n",
    "\n",
    "在训练过程中，模型保存在当前目录checkpoints文件夹下。在训练的同时在验证集上进行评估，输出`loss`和`PPL`等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "\r\n",
    "# 定义训练模型保存函数\r\n",
    "def save_ckpt(model, tokenizer, save_dir, name):\r\n",
    "    output_dir = os.path.join(save_dir, \"model_{}\".format(name))\r\n",
    "    if not os.path.exists(output_dir):\r\n",
    "        os.makedirs(output_dir)\r\n",
    "    model.save_pretrained(output_dir)\r\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "# 定义模型评估函数，在模型训练过程中会在开发集上对模型进行评估\r\n",
    "@paddle.no_grad()\r\n",
    "def evaluation(model, data_loader):\r\n",
    "    print('\\nEval begin...')\r\n",
    "    model.eval()\r\n",
    "    total_tokens = 0\r\n",
    "    total_loss = 0.0\r\n",
    "    start_time = time.time()\r\n",
    "    step = 0\r\n",
    "    for inputs in data_loader:\r\n",
    "        step += 1\r\n",
    "        token_ids, type_ids, pos_ids, generation_mask, tgt_label, tgt_pos = inputs\r\n",
    "\r\n",
    "        logits = model(token_ids, type_ids, pos_ids, generation_mask, tgt_pos)\r\n",
    "        loss = F.cross_entropy(logits, tgt_label, reduction='sum')\r\n",
    "\r\n",
    "        total_loss += loss.numpy()[0]\r\n",
    "        total_tokens += tgt_label.shape[0]\r\n",
    "\r\n",
    "    avg_loss = total_loss / total_tokens\r\n",
    "    ppl = math.exp(avg_loss)\r\n",
    "    avg_speed = (time.time() - start_time) / step\r\n",
    "    print('loss: %.4f - ppl: %.4f - %.3fs/step\\n' % (avg_loss, ppl, avg_speed))\r\n",
    "    model.train()\r\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\r\n",
    "from paddle.optimizer.lr import NoamDecay\r\n",
    "from paddle.optimizer import AdamW\r\n",
    "\r\n",
    "# 学习率\r\n",
    "lr = 5e-6\r\n",
    "# 学习率逐渐升高到基础学习率（即上面配置的lr）所需要的迭代数\r\n",
    "warmup_steps = 4000\r\n",
    "# AdamW优化器中使用的weight_decay的系数\r\n",
    "weight_decay = 0.01\r\n",
    "# 度裁剪允许的最大梯度值\r\n",
    "max_grad_norm = 0.1\r\n",
    "\r\n",
    "# 初始化Noam衰减学习率的策略\r\n",
    "lr_scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=lr, T_max=1)\r\n",
    "# 对偏置和LayerNorm层不进行weight_decay策略\r\n",
    "decay_params = [\r\n",
    "    p.name for n, p in model.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "# 初始化AdamW优化器\r\n",
    "optimizer = AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params,\r\n",
    "    grad_clip=nn.ClipGradByGlobalNorm(max_grad_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "step 1000 - loss: 3.3830 - ppl: 29.4585 - lr: 0.0000050 - 0.511s/step\n",
      "step 2000 - loss: 3.4917 - ppl: 32.8412 - lr: 0.0000050 - 0.511s/step\n",
      "step 3000 - loss: 3.6165 - ppl: 37.2067 - lr: 0.0000050 - 0.508s/step\n",
      "step 4000 - loss: 3.6260 - ppl: 37.5637 - lr: 0.0000050 - 0.512s/step\n",
      "step 5000 - loss: 3.5554 - ppl: 35.0009 - lr: 0.0000050 - 0.508s/step\n",
      "step 6000 - loss: 3.6474 - ppl: 38.3731 - lr: 0.0000050 - 0.511s/step\n",
      "step 7000 - loss: 3.7118 - ppl: 40.9292 - lr: 0.0000050 - 0.510s/step\n",
      "step 8000 - loss: 3.5451 - ppl: 34.6430 - lr: 0.0000050 - 0.510s/step\n",
      "step 9000 - loss: 3.5486 - ppl: 34.7640 - lr: 0.0000050 - 0.511s/step\n",
      "step 10000 - loss: 3.6207 - ppl: 37.3655 - lr: 0.0000050 - 0.509s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.4666 - ppl: 32.0284 - 0.168s/step\n",
      "\n",
      "step 11000 - loss: 3.6021 - ppl: 36.6740 - lr: 0.0000050 - 0.511s/step\n",
      "step 12000 - loss: 3.6242 - ppl: 37.4945 - lr: 0.0000050 - 0.507s/step\n",
      "step 13000 - loss: 3.4852 - ppl: 32.6279 - lr: 0.0000050 - 0.505s/step\n",
      "step 14000 - loss: 4.0312 - ppl: 56.3299 - lr: 0.0000050 - 0.502s/step\n",
      "step 15000 - loss: 3.9080 - ppl: 49.7996 - lr: 0.0000050 - 0.496s/step\n",
      "step 16000 - loss: 4.1156 - ppl: 61.2915 - lr: 0.0000050 - 0.497s/step\n",
      "step 17000 - loss: 3.9573 - ppl: 52.3146 - lr: 0.0000050 - 0.495s/step\n",
      "step 18000 - loss: 3.9234 - ppl: 50.5729 - lr: 0.0000050 - 0.494s/step\n",
      "step 19000 - loss: 3.9535 - ppl: 52.1163 - lr: 0.0000050 - 0.494s/step\n",
      "step 20000 - loss: 4.2041 - ppl: 66.9580 - lr: 0.0000050 - 0.493s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.4706 - ppl: 32.1559 - 0.168s/step\n",
      "\n",
      "step 21000 - loss: 3.3371 - ppl: 28.1364 - lr: 0.0000050 - 0.495s/step\n",
      "step 22000 - loss: 3.3906 - ppl: 29.6842 - lr: 0.0000050 - 0.496s/step\n",
      "step 23000 - loss: 3.2935 - ppl: 26.9379 - lr: 0.0000050 - 0.497s/step\n",
      "step 24000 - loss: 3.3409 - ppl: 28.2434 - lr: 0.0000050 - 0.495s/step\n",
      "step 25000 - loss: 3.4453 - ppl: 31.3522 - lr: 0.0000050 - 0.495s/step\n",
      "step 26000 - loss: 3.3440 - ppl: 28.3333 - lr: 0.0000050 - 0.493s/step\n",
      "step 27000 - loss: 3.3856 - ppl: 29.5358 - lr: 0.0000050 - 0.495s/step\n",
      "step 28000 - loss: 3.3763 - ppl: 29.2612 - lr: 0.0000050 - 0.496s/step\n",
      "step 29000 - loss: 3.4597 - ppl: 31.8083 - lr: 0.0000050 - 0.495s/step\n",
      "step 30000 - loss: 3.3557 - ppl: 28.6650 - lr: 0.0000050 - 0.495s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.4959 - ppl: 32.9809 - 0.167s/step\n",
      "\n",
      "step 31000 - loss: 3.2937 - ppl: 26.9412 - lr: 0.0000050 - 0.494s/step\n",
      "step 32000 - loss: 3.4241 - ppl: 30.6941 - lr: 0.0000050 - 0.495s/step\n",
      "step 33000 - loss: 3.3675 - ppl: 29.0055 - lr: 0.0000050 - 0.496s/step\n",
      "step 34000 - loss: 3.4307 - ppl: 30.8992 - lr: 0.0000050 - 0.494s/step\n",
      "step 35000 - loss: 3.3759 - ppl: 29.2492 - lr: 0.0000050 - 0.494s/step\n",
      "step 36000 - loss: 3.3863 - ppl: 29.5560 - lr: 0.0000050 - 0.495s/step\n",
      "step 37000 - loss: 3.3818 - ppl: 29.4237 - lr: 0.0000050 - 0.495s/step\n",
      "step 38000 - loss: 3.3984 - ppl: 29.9172 - lr: 0.0000050 - 0.494s/step\n",
      "step 39000 - loss: 3.3330 - ppl: 28.0234 - lr: 0.0000050 - 0.497s/step\n",
      "step 40000 - loss: 3.3547 - ppl: 28.6371 - lr: 0.0000050 - 0.494s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.5180 - ppl: 33.7179 - 0.168s/step\n",
      "\n",
      "step 41000 - loss: 3.2400 - ppl: 25.5328 - lr: 0.0000050 - 0.494s/step\n",
      "step 42000 - loss: 3.2199 - ppl: 25.0247 - lr: 0.0000050 - 0.494s/step\n",
      "step 43000 - loss: 3.2586 - ppl: 26.0142 - lr: 0.0000050 - 0.494s/step\n",
      "step 44000 - loss: 3.4778 - ppl: 32.3878 - lr: 0.0000050 - 0.496s/step\n",
      "step 45000 - loss: 3.3879 - ppl: 29.6037 - lr: 0.0000050 - 0.494s/step\n",
      "step 46000 - loss: 3.3968 - ppl: 29.8671 - lr: 0.0000050 - 0.494s/step\n",
      "step 47000 - loss: 3.3409 - ppl: 28.2433 - lr: 0.0000050 - 0.493s/step\n",
      "step 48000 - loss: 3.3063 - ppl: 27.2845 - lr: 0.0000050 - 0.494s/step\n",
      "step 49000 - loss: 3.2780 - ppl: 26.5222 - lr: 0.0000050 - 0.495s/step\n",
      "step 50000 - loss: 3.2970 - ppl: 27.0306 - lr: 0.0000050 - 0.493s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.5338 - ppl: 34.2551 - 0.167s/step\n",
      "\n",
      "step 51000 - loss: 3.3120 - ppl: 27.4390 - lr: 0.0000050 - 0.494s/step\n",
      "step 52000 - loss: 3.3427 - ppl: 28.2946 - lr: 0.0000050 - 0.494s/step\n",
      "step 53000 - loss: 3.3572 - ppl: 28.7077 - lr: 0.0000050 - 0.494s/step\n",
      "step 54000 - loss: 3.3170 - ppl: 27.5768 - lr: 0.0000050 - 0.496s/step\n",
      "step 55000 - loss: 3.2385 - ppl: 25.4959 - lr: 0.0000050 - 0.494s/step\n",
      "step 56000 - loss: 3.3628 - ppl: 28.8703 - lr: 0.0000050 - 0.494s/step\n",
      "step 57000 - loss: 3.3848 - ppl: 29.5118 - lr: 0.0000050 - 0.494s/step\n",
      "step 58000 - loss: 3.3653 - ppl: 28.9431 - lr: 0.0000050 - 0.493s/step\n",
      "step 59000 - loss: 3.3664 - ppl: 28.9726 - lr: 0.0000050 - 0.494s/step\n",
      "step 60000 - loss: 3.2602 - ppl: 26.0559 - lr: 0.0000050 - 0.496s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.5442 - ppl: 34.6136 - 0.168s/step\n",
      "\n",
      "step 61000 - loss: 3.2686 - ppl: 26.2756 - lr: 0.0000050 - 0.494s/step\n",
      "step 62000 - loss: 3.2841 - ppl: 26.6853 - lr: 0.0000050 - 0.495s/step\n",
      "step 63000 - loss: 3.2415 - ppl: 25.5716 - lr: 0.0000050 - 0.493s/step\n",
      "step 64000 - loss: 3.4365 - ppl: 31.0795 - lr: 0.0000050 - 0.494s/step\n",
      "step 65000 - loss: 3.3221 - ppl: 27.7188 - lr: 0.0000050 - 0.496s/step\n",
      "step 66000 - loss: 3.3119 - ppl: 27.4370 - lr: 0.0000050 - 0.494s/step\n",
      "step 67000 - loss: 3.2852 - ppl: 26.7149 - lr: 0.0000050 - 0.494s/step\n",
      "step 68000 - loss: 3.4018 - ppl: 30.0182 - lr: 0.0000050 - 0.494s/step\n",
      "step 69000 - loss: 3.4064 - ppl: 30.1576 - lr: 0.0000050 - 0.494s/step\n",
      "step 70000 - loss: 3.3289 - ppl: 27.9081 - lr: 0.0000050 - 0.496s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.5562 - ppl: 35.0295 - 0.168s/step\n",
      "\n",
      "step 71000 - loss: 3.4410 - ppl: 31.2180 - lr: 0.0000050 - 0.494s/step\n",
      "step 72000 - loss: 3.5238 - ppl: 33.9123 - lr: 0.0000050 - 0.494s/step\n",
      "step 73000 - loss: 3.4865 - ppl: 32.6700 - lr: 0.0000050 - 0.493s/step\n",
      "step 74000 - loss: 3.3709 - ppl: 29.1033 - lr: 0.0000050 - 0.494s/step\n",
      "step 75000 - loss: 3.3955 - ppl: 29.8294 - lr: 0.0000050 - 0.494s/step\n",
      "step 76000 - loss: 3.2145 - ppl: 24.8915 - lr: 0.0000050 - 0.496s/step\n",
      "step 77000 - loss: 3.2759 - ppl: 26.4672 - lr: 0.0000050 - 0.494s/step\n",
      "step 78000 - loss: 3.4660 - ppl: 32.0097 - lr: 0.0000050 - 0.494s/step\n",
      "step 79000 - loss: 3.3478 - ppl: 28.4394 - lr: 0.0000050 - 0.493s/step\n",
      "step 80000 - loss: 3.4338 - ppl: 30.9949 - lr: 0.0000050 - 0.493s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.5623 - ppl: 35.2452 - 0.167s/step\n",
      "\n",
      "step 81000 - loss: 3.5640 - ppl: 35.3028 - lr: 0.0000050 - 0.496s/step\n",
      "step 82000 - loss: 3.2246 - ppl: 25.1425 - lr: 0.0000050 - 0.510s/step\n",
      "step 83000 - loss: 3.3784 - ppl: 29.3236 - lr: 0.0000050 - 0.513s/step\n",
      "step 84000 - loss: 3.4169 - ppl: 30.4754 - lr: 0.0000050 - 0.512s/step\n",
      "step 85000 - loss: 3.3000 - ppl: 27.1130 - lr: 0.0000050 - 0.511s/step\n",
      "step 86000 - loss: 3.3797 - ppl: 29.3619 - lr: 0.0000050 - 0.512s/step\n",
      "step 87000 - loss: 3.1259 - ppl: 22.7793 - lr: 0.0000050 - 0.514s/step\n",
      "step 88000 - loss: 3.3502 - ppl: 28.5084 - lr: 0.0000050 - 0.511s/step\n",
      "step 89000 - loss: 3.3882 - ppl: 29.6129 - lr: 0.0000050 - 0.514s/step\n",
      "step 90000 - loss: 3.4848 - ppl: 32.6172 - lr: 0.0000050 - 0.511s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.6064 - ppl: 36.8349 - 0.168s/step\n",
      "\n",
      "step 91000 - loss: 3.1326 - ppl: 22.9340 - lr: 0.0000050 - 0.511s/step\n",
      "step 92000 - loss: 3.5349 - ppl: 34.2900 - lr: 0.0000050 - 0.513s/step\n",
      "step 93000 - loss: 3.4448 - ppl: 31.3383 - lr: 0.0000050 - 0.515s/step\n",
      "step 94000 - loss: 3.3699 - ppl: 29.0766 - lr: 0.0000050 - 0.517s/step\n",
      "step 95000 - loss: 3.2995 - ppl: 27.0980 - lr: 0.0000050 - 0.515s/step\n",
      "step 96000 - loss: 3.5859 - ppl: 36.0859 - lr: 0.0000050 - 0.516s/step\n",
      "step 97000 - loss: 3.2126 - ppl: 24.8435 - lr: 0.0000050 - 0.515s/step\n",
      "step 98000 - loss: 3.1836 - ppl: 24.1324 - lr: 0.0000050 - 0.517s/step\n",
      "step 99000 - loss: 3.3775 - ppl: 29.2965 - lr: 0.0000050 - 0.517s/step\n",
      "step 100000 - loss: 3.2215 - ppl: 25.0662 - lr: 0.0000050 - 0.515s/step\n",
      "\n",
      "Eval begin...\n",
      "loss: 3.6197 - ppl: 37.3267 - 0.169s/step\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa1f4391744e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-322>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-320>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    224\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         optimize_ops = self._apply_optimize(\n\u001b[0;32m--> 367\u001b[0;31m             loss=None, startup_program=None, params_grads=params_grads)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/optimizer/optimizer.py\u001b[0m in \u001b[0;36m_apply_optimize\u001b[0;34m(self, loss, startup_program, params_grads)\u001b[0m\n\u001b[1;32m    770\u001b[0m                                framework.default_startup_program()):\n\u001b[1;32m    771\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_clip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m                     \u001b[0mparams_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m                 params_grads = append_regularization_ops(params_grads,\n\u001b[1;32m    774\u001b[0m                                                          self.regularization)\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/clip.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, params_grads)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_dygraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dygraph_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-218>\u001b[0m in \u001b[0;36m_dygraph_clip\u001b[0;34m(self, params_grads)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_switch_tracer_mode_guard_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/clip.py\u001b[0m in \u001b[0;36m_dygraph_clip\u001b[0;34m(self, params_grads)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mmerge_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_from_selected_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0msquare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0msum_square\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0msum_square_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_square\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/nn.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input, dim, keep_dim, name)\u001b[0m\n\u001b[1;32m   4392\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m         return core.ops.reduce_sum(input, 'dim', dim, 'keep_dim', keep_dim,\n\u001b[0;32m-> 4394\u001b[0;31m                                    'reduce_all', reduce_all)\n\u001b[0m\u001b[1;32m   4395\u001b[0m     attrs = {\n\u001b[1;32m   4396\u001b[0m         \u001b[0;34m'dim'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 训练轮次\n",
    "epochs = 5\n",
    "# 日志打印间隔\n",
    "logging_steps = 1000\n",
    "# 模型保存及评估间隔\n",
    "save_steps = 10000\n",
    "# 模型的保存路径\n",
    "save_dir = './checkpoints/'\n",
    "\n",
    "step = 0\n",
    "total_time = 0.0\n",
    "best_ppl = 100.0\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch %d/%d' % (epoch + 1, epochs))\n",
    "    batch_start_time = time.time()\n",
    "    for inputs in train_dataloader:\n",
    "        step += 1\n",
    "        token_ids, type_ids, pos_ids, generation_mask, tgt_label, tgt_pos = inputs\n",
    "\n",
    "        logits = model(token_ids, type_ids, pos_ids, generation_mask, tgt_pos)\n",
    "        # 使用交叉熵损失函数计算loss\n",
    "        loss = F.cross_entropy(logits, tgt_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        total_time += (time.time() - batch_start_time)\n",
    "        if step % logging_steps == 0:\n",
    "            ppl = paddle.exp(loss)\n",
    "            print('step %d - loss: %.4f - ppl: %.4f - lr: %.7f - %.3fs/step'\n",
    "                % (step, loss, ppl, optimizer.get_lr(), total_time / logging_steps))\n",
    "            total_time = 0.0\n",
    "        if step % save_steps == 0:\n",
    "            # 在开发集上对模型进行评估\n",
    "            tmp_ppl = evaluation(model, valid_dataloader)\n",
    "            \n",
    "            if tmp_ppl < best_ppl:\n",
    "                save_ckpt(model, tokenizer, save_dir, \"best\")\n",
    "\n",
    "            # 保存模型\n",
    "            save_ckpt(model, tokenizer, save_dir, step)\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "print(\"\\n=====training complete=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速搭建基线Step5：预测解码\n",
    "\n",
    "用训练保存的模型参数来初始化模型，加载测试集后即可进行预测。\n",
    "\n",
    "**PaddleNLP针对生成式任务提供了`generate`函数，支持Greedy Search、Beam Search和Sampling解码策略，用户只需指定解码策略以及相应的参数即可完成预测解码，得到生成的sequence的token ids以及概率得分。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这里可以是paddlenlp提供的预训练模型名称，或者自己训练获得的微调模型路径\r\n",
    "model_name_or_path = './checkpoints/model_100000' \r\n",
    "# 加载模型\r\n",
    "model = UnifiedTransformerLMHeadModel.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预测batch_size\r\n",
    "from paddle.io import DataLoader\r\n",
    "from data import DialogueDataset\r\n",
    "batch_size = 8\r\n",
    "\r\n",
    "# 测试集路径，注意与数据预处理输出路径保持一致\r\n",
    "test_data_path = './data/test.txt' \r\n",
    "test_dataset = DialogueDataset(\r\n",
    "    test_data_path,\r\n",
    "    batch_size,\r\n",
    "    tokenizer.pad_token_id,\r\n",
    "    tokenizer.cls_token_id,\r\n",
    "    mode='test')\r\n",
    "test_dataloader = DataLoader(test_dataset, return_list=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Infer begin...\n"
     ]
    }
   ],
   "source": [
    "from data import select_response\r\n",
    "import time\r\n",
    "\r\n",
    "# 预测解码生成序列的最大长度\r\n",
    "max_dec_len = 64\r\n",
    "# 预测解码生成序列的最小长度\r\n",
    "min_dec_len = 1\r\n",
    "# 解码策略\r\n",
    "decode_strategy = 'sampling'\r\n",
    "# topk-sampling解码参数top_k\r\n",
    "top_k = 5\r\n",
    "# 每条输入序列返回的输出序列个数，生成式API内部会将输入序列进行复制\r\n",
    "num_return_sequences = 20\r\n",
    "# 文本结果序列保存路径\r\n",
    "output_path = './predict.txt'\r\n",
    "# 日志打印间隔\r\n",
    "logging_steps = 50\r\n",
    "\r\n",
    "print('\\nInfer begin...')\r\n",
    "model.eval()\r\n",
    "total_time = 0.0\r\n",
    "start_time = time.time()\r\n",
    "responses = []\r\n",
    "for step, inputs in enumerate(test_dataloader, 1):\r\n",
    "    input_ids, token_type_ids, position_ids, attention_mask = inputs\r\n",
    "    ids, scores = model.generate(\r\n",
    "        input_ids=input_ids,\r\n",
    "        token_type_ids=token_type_ids,\r\n",
    "        position_ids=position_ids,\r\n",
    "        attention_mask=attention_mask,\r\n",
    "        max_length=max_dec_len,\r\n",
    "        min_length=min_dec_len,\r\n",
    "        decode_strategy=decode_strategy,\r\n",
    "        top_k=top_k,\r\n",
    "        num_return_sequences=num_return_sequences)\r\n",
    "\r\n",
    "    total_time += (time.time() - start_time)\r\n",
    "    if step % logging_steps == 0:\r\n",
    "        print('step %d - %.3fs/step' % (step, total_time / logging_steps))\r\n",
    "        total_time = 0.0\r\n",
    "    # 模型输出序列排序，从num_return_sequences个序列中选出最好的一个作为结果\r\n",
    "    results = select_response(ids, scores, tokenizer, max_dec_len, num_return_sequences)\r\n",
    "    responses.extend(results)\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "\r\n",
    "# 保存文本结果序列\r\n",
    "with open(output_path, 'w', encoding='utf-8') as fout:\r\n",
    "    for response in responses:\r\n",
    "        fout.write(response + '\\n')\r\n",
    "print('\\nSave inference result into: %s' % output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 快速搭建基线Step6：提交结果\n",
    "\n",
    "预测结果会被保存在`output_path`中，将预测结果准备成比赛官网要求的格式，提交到[比赛官网](https://aistudio.baidu.com/aistudio/competition/detail/67)进行评测即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上基线实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
