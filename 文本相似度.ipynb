{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlehub              2.0.4          \r\n",
      "paddlenlp              2.0.5          \r\n",
      "paddlepaddle-gpu       2.1.0.post101  \r\n",
      "tb-paddle              0.3.6          \r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前使用 paddle以及生态工具的版本\n",
    "!pip list | grep paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 比赛结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 任务 & 数据集\n",
    "文本相似度：https://aistudio.baidu.com/aistudio/competition/detail/45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据浏览 & 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压数据集, 如果存在跳过此步骤\n",
    "# !unzip -q data/data52714/bq_corpus.zip \n",
    "# !unzip -q data/data52714/lcqmc.zip \n",
    "# !unzip -q data/data52714/paws-x-zh.zip\n",
    "\n",
    "# print(\"--- lcqmc ----\")\n",
    "# !head -n 5 lcqmc/train.tsv\n",
    "# print(\"---bq_corpus----\")\n",
    "# !head -n 5 bq_corpus/train.tsv\n",
    "# print(\"---paws-x-zh----\")\n",
    "# !head -n 5 paws-x-zh/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def get_ds(ds_name='lcqmc'):\n",
    "    \"\"\"\n",
    "    lcqmc、bq_corpus、paws-x-zh\n",
    "    \"\"\"\n",
    "\n",
    "    train_file = '{}/train.tsv'.format(ds_name)\n",
    "    dev_file = '{}/dev.tsv'.format(ds_name)\n",
    "    test_file = '{}/test.tsv'.format(ds_name)\n",
    "    \n",
    "    train_ds, dev_ds, test_ds= load_dataset(\"lcqmc\", data_files=[train_file, dev_file, test_file])\n",
    "\n",
    "    return train_ds, dev_ds, test_ds\n",
    "\n",
    "# 注意每次选择一个数据集进行跑\n",
    "# data_name = 'lcqmc'\n",
    "\n",
    "# data_name = 'bq_corpus'\n",
    "\n",
    "# 注意 paws-x-zh 数据集中，有些行的数据仅有一列数据，所以需要修改 paddlenlp.datasets.lcqmc.py \n",
    "# 请参见：https://github.com/PaddlePaddle/PaddleNLP/pull/553/files\n",
    "data_name = 'paws-x-zh'\n",
    "\n",
    "train_ds, dev_ds, test_ds = get_ds(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ds.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预训练模型列表：https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/model_zoo/transformers.rst\n",
    "\n",
    "# ERNIE-GRAM\n",
    "# tokenizer = paddlenlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "# RoBERTa\n",
    "tokenizer = paddlenlp.transformers.RobertaTokenizer.from_pretrained('roberta-wwm-ext-large')\n",
    "\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "\n",
    "    query, title = example[\"query\"], example[\"title\"]\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=query, text_pair=title, max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    # 在预测或者评估阶段，不返回 label 字段\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "# 为了后续方便使用，我们使用python偏函数（partial）给 convert_example 赋予一些默认参数\n",
    "from functools import partial\n",
    "from paddlenlp.data import Tuple, Pad, Stack\n",
    "\n",
    "\n",
    "# 训练集和验证集的样本转换函数\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512)\n",
    "\n",
    "# 我们的训练数据会返回 input_ids, token_type_ids, labels 3 个字段\n",
    "# 因此针对这 3 个字段需要分别定义 3 个组 batch 操作\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "# 定义分布式 Sampler: 自动对训练数据进行切分，支持多卡并行训练\n",
    "# batch_size 设置的越大，训练速度越快，同时显存占用也越多，注意平衡\n",
    "batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=32, shuffle=True) #128\n",
    "\n",
    "# 基于 train_ds 定义 train_data_loader\n",
    "# 因为我们使用了分布式的 DistributedBatchSampler, train_data_loader 会自动对训练数据进行切分\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "        dataset=train_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "# 针对验证集数据加载，我们使用单卡进行评估，所以采用 paddle.io.BatchSampler 即可\n",
    "# 定义 dev_data_loader\n",
    "batch_sampler = paddle.io.BatchSampler(dev_ds, batch_size=16, shuffle=False)\n",
    "dev_data_loader = paddle.io.DataLoader(\n",
    "        dataset=dev_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# 测试集\n",
    "# 注意: predict 数据没有 label, 因此 convert_exmaple 的 is_test 参数设为 True\n",
    "# =======================================================================\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    is_test=True)\n",
    "\n",
    "# 预测数据的组 batch 操作\n",
    "# predict 数据只返回 input_ids 和 token_type_ids，因此只需要 2 个 Pad 对象作为 batchify_fn\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment_ids\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# 生成预测数据 data_loader\n",
    "predict_data_loader =paddle.io.DataLoader(\n",
    "        dataset=test_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ds.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 建模\n",
    "详情参考：https://github.com/PaddlePaddle/PaddleNLP/blob/v2.0.2/examples/text_matching/ernie_matching/train_pointwise.py\n",
    "* 注意： paddlenlp == 2.0.2\n",
    "\n",
    "主要由以下几种实现方式：\n",
    " 1. 基于单塔 Point-wise 范式的语义匹配模型 ernie_matching: 模型精度高、计算复杂度高, 适合直接进行语义匹配 2 分类的应用场景。\n",
    " 2. 基于单塔 Pair-wise 范式的语义匹配模型 ernie_matching: 模型精度高、计算复杂度高, 对文本相似度大小的序关系建模能力更强，适合将相似度特征作为上层排序模块输入特征的应用场景。\n",
    " 3. 基于双塔 Point-wise 范式的语义匹配模型 SimNet 和 Sentence Transformers, 这 2 种方案计算效率更高，适合对延时要求高、根据语义相似度进行粗排的应用场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基于单塔 Point-wise 范式的语义匹配模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 环境 init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "device = paddle.get_device()\n",
    "paddle.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 预训练模型列表：https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/model_zoo/transformers.rst\n",
    "\n",
    "# ERNIE-GRAM\n",
    "# pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "# RoBERTa \n",
    "pretrained_model = paddlenlp.transformers.RobertaModel.from_pretrained('roberta-wwm-ext-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 定义 Point-wise 语义匹配网络\n",
    "class PointwiseMatching(nn.Layer):\n",
    "\n",
    "    def __init__(self, pretrained_model, dropout=None):\n",
    "        super().__init__()\n",
    "        self.ptm = pretrained_model\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # 语义匹配任务: 相似、不相似 2 分类任务\n",
    "        self.classifier = nn.Linear(self.ptm.config[\"hidden_size\"], 2)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None):\n",
    "\n",
    "        # 此处的 Input_ids 由两条文本的 token ids 拼接而成\n",
    "        # token_type_ids 表示两段文本的类型编码\n",
    "        # 返回的 cls_embedding 就表示这两段文本经过模型的计算之后而得到的语义表示向量\n",
    "        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,\n",
    "                                    attention_mask)\n",
    "\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "\n",
    "        # 基于文本对的语义表示向量进行 2 分类任务\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PointwiseMatching(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 设置优化策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "epochs = 10\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "lr_val = paddle.optimizer.lr.MultiStepDecay(learning_rate=5e-6, milestones=[3, 6, 9], gamma=0.5, verbose=True)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_val,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.001,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 采用交叉熵 损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, phase=\"dev\"):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        loss = criterion(probs, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval {} loss: {:.5}, accu: {:.5}\".format(phase,\n",
    "                                                    np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定 best model 保存路径\r\n",
    "model_file = \"{}/model/model_state.pdparams\".format(data_name)\r\n",
    "# model_file = \"{}/roberta_model/model_state.pdparams\".format(data_name)\r\n",
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "import time \n",
    "import os\n",
    "\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "max_acc = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # trick：我们将之前的模型 load 后，这里可以使用 dev_data_loader 再训练一下参数\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        loss = criterion(probs, labels)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "        # 每间隔 10 step 输出训练指标\n",
    "        if global_step % 100 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # 每间隔 100 step 在验证集和测试集上进行评估\n",
    "        if global_step % 200 == 0:\n",
    "            acc = evaluate(model, criterion, metric, dev_data_loader, \"dev\")\n",
    "                \n",
    "            if acc >= 0.85 and acc > max_acc:\n",
    "                print(\"current step: {}, acc: {}\".format(global_step, acc))\n",
    "                # model_file = \"{}/model_{}/model_state.pdparams\".format(data_name, global_step)\n",
    "                if os.path.exists(model_file):\n",
    "                    os.remove(model_file)\n",
    "                paddle.save(model.state_dict(), model_file)\n",
    "                max_acc = acc\n",
    "    \n",
    "    epoch_model_file = \"{}/model_{}/model_state.pdparams\".format(data_name, epoch)\n",
    "    if os.path.exists(epoch_model_file):\n",
    "        os.remove(epoch_model_file)\n",
    "    paddle.save(model.state_dict(), epoch_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \n",
    "    batch_probs = []\n",
    "\n",
    "    # 预测阶段打开 eval 模式，模型中的 dropout 等操作会关掉\n",
    "    model.eval()\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        for batch_data in data_loader:\n",
    "            input_ids, token_type_ids = batch_data\n",
    "            input_ids = paddle.to_tensor(input_ids)\n",
    "            token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "            \n",
    "            # 获取每个样本的预测概率: [batch_size, 2] 的矩阵\n",
    "            batch_prob = model(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids).numpy()\n",
    "\n",
    "            batch_probs.append(batch_prob)\n",
    "        batch_probs = np.concatenate(batch_probs, axis=0)\n",
    "\n",
    "        return batch_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-13 11:07:21,991] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large/roberta_chn_large.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 可选步骤，创建模型，并选择最好的模型进行加载\n",
    "# ERNIE-GRAM\n",
    "# pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "# RoBERTa \n",
    "pretrained_model = paddlenlp.transformers.RobertaModel.from_pretrained('roberta-wwm-ext-large')\n",
    "\n",
    "model = PointwiseMatching(pretrained_model)\n",
    "\n",
    "state_dict = paddle.load(model_file)\n",
    "model.set_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 执行预测函数\n",
    "y_probs = predict(model, predict_data_loader)\n",
    "\n",
    "# 根据预测概率获取预测 label\n",
    "y_preds = np.argmax(y_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\r\n",
    "# if os.path.exists(\"submit.zip\"):\r\n",
    "#     os.remove(\"submit.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 输出预测结果\n",
    "* 满足比赛的输出格式 （https://aistudio.baidu.com/aistudio/competition/detail/45）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"{}/{}.tsv\".format(data_name, data_name)\n",
    "print(\"will generate file: \", file_name)\n",
    "with open(file_name, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")    \n",
    "    for idx, y_pred in enumerate(y_preds):\n",
    "        f.write(\"{}\\t{}\\n\".format(idx, y_pred))\n",
    "        # text_pair = test_ds.data[idx]\n",
    "        # text_pair[\"label\"] = y_pred\n",
    "        # if  idx <= 10:\n",
    "        #     print(text_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 打包预测结果，提交至比赛处\n",
    "* 当所有的数据（lcqmc、bq_corpus、paws-x-zh）都分别训练，并产出预测文件后（lcqmc.tsv、paws-x.tsv、bq_corpus.tsv），我们将其打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: lcqmc.tsv (deflated 65%)\r\n",
      "  adding: paws-x.tsv (deflated 64%)\r\n",
      "  adding: bq_corpus.tsv (deflated 65%)\r\n"
     ]
    }
   ],
   "source": [
    "# # 打包预测结果\n",
    "# !mv paws-x-zh.tsv paws-x.tsv\n",
    "# !zip submit.zip lcqmc.tsv paws-x.tsv bq_corpus.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\r\n",
    "    model_file = data_name + \"/model_\" + str(i+1) + \"/model_state.pdparams\"\r\n",
    "    # pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\r\n",
    "\r\n",
    "    # RoBERTa \r\n",
    "    pretrained_model = paddlenlp.transformers.RobertaModel.from_pretrained('roberta-wwm-ext-large')\r\n",
    "\r\n",
    "    model = PointwiseMatching(pretrained_model)\r\n",
    "    state_dict = paddle.load(model_file)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "\r\n",
    "    # 执行预测函数\r\n",
    "    y_probs = predict(model, predict_data_loader)\r\n",
    "\r\n",
    "    # 根据预测概率获取预测 label\r\n",
    "    y_preds = np.argmax(y_probs, axis=1)\r\n",
    "\r\n",
    "    file_name = data_name + \"/out_files_ro/{}.tsv\".format(str(i))\r\n",
    "    print(\"will generate file: \", file_name)\r\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as f:\r\n",
    "        f.write(\"index\\tprediction\\n\")    \r\n",
    "        for idx, y_pred in enumerate(y_preds):\r\n",
    "            f.write(\"{}\\t{}\\n\".format(idx, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcqmc/out_files_ro/0.tsv bq_corpus/out_files_ro/0.tsv paws-x-zh/out_files_ro/0.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/1.tsv bq_corpus/out_files_ro/1.tsv paws-x-zh/out_files_ro/1.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/2.tsv bq_corpus/out_files_ro/2.tsv paws-x-zh/out_files_ro/2.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/3.tsv bq_corpus/out_files_ro/3.tsv paws-x-zh/out_files_ro/3.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/4.tsv bq_corpus/out_files_ro/4.tsv paws-x-zh/out_files_ro/4.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/5.tsv bq_corpus/out_files_ro/5.tsv paws-x-zh/out_files_ro/5.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/6.tsv bq_corpus/out_files_ro/6.tsv paws-x-zh/out_files_ro/6.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/7.tsv bq_corpus/out_files_ro/7.tsv paws-x-zh/out_files_ro/7.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/8.tsv bq_corpus/out_files_ro/8.tsv paws-x-zh/out_files_ro/8.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n",
      "lcqmc/out_files_ro/9.tsv bq_corpus/out_files_ro/9.tsv paws-x-zh/out_files_ro/9.tsv\n",
      "  adding: submission/bq_corpus.tsv (deflated 65%)\n",
      "  adding: submission/lcqmc.tsv (deflated 65%)\n",
      "  adding: submission/paws-x.tsv (deflated 64%)\n"
     ]
    }
   ],
   "source": [
    "!python create_zip.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
